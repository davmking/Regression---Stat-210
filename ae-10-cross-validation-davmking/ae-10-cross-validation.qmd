---
title: "AE 10: Cross validation"
subtitle: "The Office"
date: "Oct 24, 2022"
format: pdf
editor: visual
---

::: callout-important
This AE is due on GitHub by Thursday, October 27 at 11:59pm.
:::

## Function

```{r}
# function to calculate model fit statistics
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

## Packages

```{r}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
```

## Load data

```{r}
#| label: load-data
#| message: false

office_episodes <- read_csv("data/office_episodes.csv")
```

## Split data into training and testing

Split your data into testing and training sets.

```{r}
#| label: initial-split

set.seed(123)
office_split <- initial_split(office_episodes)
office_train <- training(office_split)
office_test <- testing(office_split)
```

## Specify model

Specify a linear regression model. Call it `office_spec`.

```{r}
#| label: specify-model

office_spec <- linear_reg() |>
  set_engine("lm")

office_spec
```

# Model 1

## Create recipe

Create the recipe from class. Call it `office_rec1`.

```{r}
#| label: create-recipe

office_rec1 <- recipe(imdb_rating ~ ., data = office_train) |>
  update_role(episode_name, new_role = "id") |>
  step_rm(air_date, season) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

office_rec1
```

## Preview recipe

```{r}
#| label: preview-recipe

prep(office_rec1) |>
  bake(office_train) |>
  glimpse()
```

## Create workflow

Create the workflow that brings together the model specification and recipe. Call it `office_wflow1`.

```{r}
#| label: create-wflow

office_wflow1 <- workflow() |>
  add_model(office_spec) |>
  add_recipe(office_rec1)

office_wflow1
```

# Cross validation

## Create folds

Create 10-folds.

```{r}
#| label: cv-tenfold

# make 10 folds
set.seed(345)
folds <- vfold_cv(office_train, v = 10)
```

## Conduct cross validation

Conduct cross validation on the 10 folds.

```{r}
#| label: conduct-cv

set.seed(456)
# Fit model and calculate statistics for each fold
office_fit_rs1 <- office_wflow1 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
```

## Summarize assessment CV metrics

Summarize assessment metrics from your CV resamples.

```{r}
#| label: cv-summarize

collect_metrics(office_fit_rs1, summarize = TRUE)
```

## Summarize model fit CV metrics

```{r}
#| label: cv-model-fit
map_df(office_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

# Another model - Model 2

Create a different (simpler, involving fewer variables) recipe and call it `office_rec2`. Conduct 10-fold cross validation and summarize metrics.

## Model 2: Recipe

```{r}
#| label: model2-recipe

office_rec2 <- recipe(imdb_rating ~ season + episode + air_date + michael, data = office_train) %>% 
  step_date(air_date)
```

## Model 2: Model building workflow

```{r}
#| label: model2-workflow

office_wflow2 <- workflow() %>% 
  add_model(office_spec) %>% 
  add_recipe(office_rec2)
```

## Model 2: Conduct CV

::: callout-note
Note: We will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?
:::

```{r}
#| label: model2-cv

set.seed(345)
office_fit_rs2 <- office_wflow2 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
```

## Model 2: Summarize assessment CV metrics

```{r}
#| label: model2-assessment

collect_metrics(office_fit_rs2, summarize = T)
```

## Model 2: Summarize model fit CV metrics

```{r}
#| label: model2-model-fit

map_df(office_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

# Compare models

The model I created performs worse on the training data than the model created above. It had a higher RMSE value (0.432 vs 0.339) and lower $R^2$ value (0.277 vs 0.567) than the original model, suggesting that it has a higher error on average and accounts for less of the variability in imdb_data. Additionally, when we look at more advanced model comparison metrics, it again performs worse. Looking at adjusted $R^2$, which calculates the amount of the variance in the response variable that can be explained by the predictors while penalizing the model for having too many predictors, even though my model has fewer predictors it still performs far worse (0.315 vs 0.583). And, it has much higher AIC and BIC values, which we generally want to lower. Thus, according to this analysis, my model performs worse than the class model.

Use RMSE and BIC for prediction, Adj $R^2$ and AIC for explanation.

# Submission

::: callout-important
To submit the AE:

-   Render the document to produce the PDF with all of your work from today's class.
-   Push all your work to your `ae-10-` repo on GitHub. (You do not submit AEs on Gradescope).
:::
