---
title: "AE 08: Feature Engineering"
subtitle: "The Office"
date: "Oct 05, 2022"
format: pdf
editor: visual
---

::: callout-important
The AE is due on GitHub by Saturday, October 08 at 11:59pm.
:::

## Packages

```{r load-packages}
#| message: false
library(tidyverse)
library(tidymodels)
library(viridis)
library(knitr)
```

## Load data

```{r}
#| message: false
office_ratings <- read_csv("data/office_ratings.csv")
```

## Exploratory data analysis

Below are two of the exploratory data analysis plots from lecture.

```{r}
ggplot(office_ratings, aes(x = imdb_rating)) +
  geom_histogram(binwidth = 0.25) +
  labs(
    title = "The Office ratings",
    x = "IMDB rating"
  )
```

```{r}
office_ratings |>
  mutate(season = as_factor(season)) |>
  ggplot(aes(x = season, y = imdb_rating, color = season)) +
  geom_boxplot() +
  geom_jitter() +
  guides(color = "none") +
  labs(
    title = "The Office ratings",
    x = "Season",
    y = "IMDB rating"
  ) +
  scale_color_viridis_d()
```

## Test/train split

```{r}
set.seed(123)
office_split <- initial_split(office_ratings) # prop = 3/4 by default
office_train <- training(office_split)
office_test  <- testing(office_split)
```

## Build a recipe

```{r}
office_rec <- recipe(imdb_rating ~ ., data = office_train) |>
  # make title's role ID
  update_role(title, new_role = "ID") |>
  # extract day of week and month of air_date
  step_date(air_date, features = c("dow", "month")) |>
  # identify holidays and add indicators
  step_holiday(
    air_date, 
    holidays = c("USThanksgivingDay", "USChristmasDay",
                 "USNewYearsDay", "USIndependenceDay"), 
    keep_original_cols = FALSE
  ) |>
  # turn season into factor
  step_num2factor(season, levels = as.character(1:9)) |>
  # make dummy variables
  step_dummy(all_nominal_predictors()) |>
  # remove zero variance predictors
  step_zv(all_predictors())
```

```{r}
#| label: view-recipe

office_rec
```

## Workflows and model fitting

### Specify model

```{r}

office_spec <- linear_reg() |>
  set_engine("lm")

office_spec
```

### Build workflow

```{r}

office_wflow <- workflow() |>
  add_model(office_spec) |>
  add_recipe(office_rec)
```

```{r}
#| label: view-workflow
office_wflow
```

### Fit model to training data

```{r}
office_fit <- office_wflow |>
  fit(data = office_train)

tidy(office_fit) |>
  kable(digits = 3)
```

## Evaluate model on training data

### Make predictions

::: callout-important
Fill in the code and make `#| eval: true` before rendering the document.
:::

```{r}
#| eval: true

office_train_pred <- predict(office_fit, office_train) |>
  bind_cols(office_train)

office_train_pred
```

### Calculate $R^2$

::: callout-important
Fill in the code and make `#| eval: true` before rendering the document.
:::

```{r}
#| eval: true
rsq(office_train_pred, truth = imdb_rating, estimate = .pred)
```

-   What is preferred - high or low values of $R^2$?

We prefer to have high values of $R^2$, because those indicate that our model explains a larger amount of the variance in our response variable.

### Calculate RMSE

::: callout-important
Fill in the code and make `#| eval: true` before rendering the document.
:::

```{r}
#| eval: false
rmse(office_train_pred, truth = imdb_rating, estimate = .pred)
```

-   What is preferred - high or low values of RMSE?

We prefer to have low values of RMSE, because this corresponds with lower error in our model.

-   Is this RMSE considered high or low? *Hint: Consider the range of the response variable to answer this question*.

This is probably considered low RMSE - it only reflects about 10% of the range in the response variable, meaning our prediction is generally pretty accurate.

    ```{r}
    office_train |>
      summarise(min = min(imdb_rating), max = max(imdb_rating))
    ```

## Evaluate model on testing data

Answer the following before evaluating the model performance on testing data:

-   Do you expect $R^2$ on the testing data to be higher or lower than the $R^2$ calculated using training data? Why?

Lower - the model was built to explain variance in the existing data, and it would be somewhat surprising if it were better at explaining variance in a completely different set of data on which it was not trained.

-   Do you expect RMSE on the testing data to be higher or lower than the $R^2$ calculated using training data? Why?

Higher - the model was built to minimize error in the prediction of IMDB ratings based on the training data we had. It would be surprising if it had even less error when predicting IMDB ratings in other data.

### Make predictions

```{r}
# fill in code to make predictions from testing data
testPred <- predict(office_fit, office_test) %>% 
  bind_cols(office_test)
```

### Calculate $R^2$

```{r}
# fill in code to calculate $R^2$ for testing data
rsq(testPred, truth = imdb_rating, estimate = .pred)
```

### Calculate RMSE

```{r}
# fill in code to calculate RMSE for testing data
rmse(testPred, truth = imdb_rating, estimate = .pred)
```

## Compare training and testing data results

-   Compare the $R^2$ for the training and testing data. Is this what you expected?

The $R^2$ of the testing data is notably lower than that of the training data. This is exactly what we would expect, because the model was not built on the testing data and thus cannot provide as accurate of a prediction.

-   Compare the RMSE for the training and testing data. Is this what you expected?

The RMSE of the testing data is notably higher than that of the training data. This is, again, exactly what we would expect - the model was not built on the training data, so it will provide predictions for the IMDB ratings of episodes in the testing data that have more error.

::: callout-important
To submit the AE:

-   Render the document to produce the PDF with all of your work from today's class.
-   Push all your work to your `ae-08-` repo on GitHub. (You do not submit AEs on Gradescope).
:::
